{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jZfaatV7xazF"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FmNpD0JDxc1"
      },
      "source": [
        "# Semantic Segmentation\n",
        "The task comprises segmenting input images so as to identify crops, weeds and soil (background). Four different picture types, called teams, are available in the dataset and for each type of picture there is a further distinction between Haricot and Mais.   \n",
        "We chose to work on Mais pictures from the Weedelec team.\n",
        "The pictures from this team are very high resolution which is one of the main challenges due to the limitation of computational capabilities in training Deep Neural Networks.  \n",
        "Our work builds on the Unet architecture by adding different modules and experimenting with different depths. On top of the architectures we devised we also created an infrastructure to train those architectures either by patching or by downsampling.  \n",
        "Patching was tested with and without overlapping patches. Overlaps were solved by taking the minimum because the background is the most frequent class in the images so taking the minimum can be beneficial when guessing the value of a pixel.  \n",
        "Downsampling instead was used so as to comply with GPU memory limit by reducing both image size and batch size and finding a balance between the two. In prediction phase we upsample the predicted masks with the nearest neighbor interpolation.  \n",
        "As a loss we chose the Sparse Focal loss because our targets are not one hot encoded so we needed a sparse loss and on top of this we also need to take into account class imbalance (see https://arxiv.org/abs/2006.14822 for further details on this).  \n",
        "As an optimizer we chose Lazy Adam which is well versed when dealing with sparse updates (see https://www.tensorflow.org/addons/tutorials/optimizers_lazyadam).  \n",
        "Network Architectures:\n",
        "For some of our network architectures we implemented the Renet layer (as seen in https://arxiv.org/abs/1511.07053) by substantially adapting the implementations found in https://github.com/fvisin/reseg and https://github.com/hydxqing/ReNet-pytorch-keras-chapter3 . Our implementation employs tensorflow and keras specific methods to deliver a performant and compatible implementation. Generally speaking we used this module so as to break down in smaller patches the incoming feature maps and get local information.  \n",
        "- UNet: depth parameter controls the number of convolutions of decoder-encoder path so with depth 4 we would have a 4 convolution decoder and a 4 convolution encoder. start_f parameter controls the size of the first convolution which is doubled at each subsequenct convolution.   \n",
        "Results\n",
        "  - input=256x256 downsampled, depth=4 start_f=32: 0.37\n",
        "  - input=512x512 tiling (patching with no overlap), depth=5 start_f=32: 0.69\n",
        "- DuccioNet: this a Unet which in the section between encoder and decoder uses three renet layers so as to better identify weed which is very high resolution information by dividing feature maps in smaller patches.  \n",
        "Results:\n",
        "  - input=1152x768 downsampled, depth=4 start_f=32: 0.64\n",
        "  - input=864x864 patching with overlap=54, depth=5 start_f=32: 0.67\n",
        "- FrebianiNet: this is a DuccioNet which has in parallel with the renet layers  three atrous convolutions which output is concatenated to that of the three renet layers. Atrous convolutions at the bottom of the net are inspired from https://ieeexplore.ieee.org/document/8999616.   \n",
        "Results:\n",
        "  - input=1152x768 downsampled, depth=4 start_f=32: 0.64\n",
        "  - input=864x864 patching with overlap=54, depth=5 start_f=32: 0.69 (should have ran for more epochs but training is very computationally expensive and hard to carry out on colab) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eggNoIkxxayr"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/Development_Dataset.zip\" .\n",
        "!mkdir Results\n",
        "!unzip -q Development_Dataset.zip\n",
        "!pip install focal-loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Txb_Lejxayv"
      },
      "source": [
        "# Cell output set up for Jupyter\n",
        "from pathlib import Path\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "from itertools import cycle\n",
        "from focal_loss import SparseCategoricalFocalLoss"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehy-JCkBxayy"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Conv2D,MaxPool2D,Cropping2D,Concatenate,Conv2DTranspose, Activation, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow_addons.layers import WeightNormalization\n",
        "\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwuQ7e2Zxay0"
      },
      "source": [
        "#Config\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swjhTJ2Wxay2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c408b7bc-3fbf-4689-dd37-b9bf97bcf9b5"
      },
      "source": [
        "dataset_name = \"Development_Dataset\"\n",
        "folder_name = \"Dataset\"\n",
        "crop_list = [\"Haricot\", \"Mais\"]\n",
        "team_list = [\"Bipbip\", \"Pead\", \"Roseau\", \"Weedelec\"]\n",
        "curr_crop = \"Mais\"\n",
        "curr_team = \"Weedelec\"\n",
        "curr_patch_size = 864\n",
        "img_h = 768\n",
        "img_w = 1152\n",
        "img_folder = \"Images\"\n",
        "ann_folder = \"Annotations\"\n",
        "ratio = 0.8\n",
        "batch_size = 4\n",
        "\n",
        "def change_config(new_width = img_w , new_height = img_h ,\n",
        "                  new_team = curr_team, new_crop = curr_crop , new_patch_size = curr_patch_size,\n",
        "                  new_ratio = ratio, new_batch_size = batch_size):\n",
        "  if new_width:\n",
        "    global img_w\n",
        "    img_w = new_width\n",
        "  if new_height:  \n",
        "    global img_h \n",
        "    img_h = new_height\n",
        "  if new_team:\n",
        "    global curr_team \n",
        "    curr_team = new_team\n",
        "  if new_crop:\n",
        "    global curr_crop\n",
        "    curr_crop = new_crop\n",
        "  if new_patch_size:\n",
        "    global curr_patch_size \n",
        "    curr_patch_size = new_patch_size\n",
        "    img_h = curr_patch_size \n",
        "    img_w = curr_patch_size \n",
        "  global img_folder\n",
        "  global ann_folder\n",
        "  img_folder = \"Image_Patches\" if new_patch_size else \"Images\"\n",
        "  ann_folder = \"Annotation_Patches\" if new_patch_size else \"Annotations\"\n",
        "  if new_ratio:\n",
        "    global ratio\n",
        "    ratio = new_ratio\n",
        "  if new_batch_size:\n",
        "    global batch_size\n",
        "    batch_size = new_batch_size\n",
        "change_config()\n",
        "def print_config():\n",
        "  print(\"The current batch size is: \" + str(batch_size) + \"\\n\"\n",
        "      \"The shape of the image is: (\" + str(img_w) + ',' + str(img_h) + \")\\n\"\n",
        "      \"The current team is: \" + curr_team + \"\\n\"\n",
        "      \"The current crop is: \" + curr_crop + \"\\n\"\n",
        "      \"The current patch size is: \" + str(curr_patch_size) + \"\\n\"\n",
        "      \"The current split ratio is: \"+ str(ratio) + \"\\n\"\n",
        "        )\n",
        "  \n",
        "print_config()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The current batch size is: 4\n",
            "The shape of the image is: (864,864)\n",
            "The current team is: Weedelec\n",
            "The current crop is: Mais\n",
            "The current patch size is: 864\n",
            "The current split ratio is: 0.8\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWpI5a7Zxay5"
      },
      "source": [
        "# Building Directory structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5DygVrnxay7"
      },
      "source": [
        "def build_directory_structure():\n",
        "  Path().joinpath(folder_name).mkdir(parents=True, exist_ok=True)\n",
        "  Path().joinpath(folder_name,\"Annotations\").mkdir(parents=True, exist_ok=True)\n",
        "  Path().joinpath(folder_name,\"Images\").mkdir(parents=True, exist_ok=True)\n",
        "  Path().joinpath(folder_name,\"Splits\").mkdir(parents=True, exist_ok=True)\n",
        "  Path().joinpath(folder_name,\"Splits\",\"train.txt\").open(\"w+\").close()\n",
        "  Path().joinpath(folder_name,\"Splits\",\"val.txt\").open(\"w+\").close()\n",
        "\n",
        "def fill_directory(team = curr_team ,crop = curr_crop ,patch = None):\n",
        "  #Checking if team and crop are specified, if not it is considered global\n",
        "  if not team:\n",
        "    if not crop:\n",
        "      for t,c in zip(team_list, cycle(crop_list)):\n",
        "        fill_directory(t,c)\n",
        "      return\n",
        "    for t in team_list:\n",
        "      fill_directory(t,crop)\n",
        "    return\n",
        "  if not crop:\n",
        "    for c in crop_list:\n",
        "      fill_directory(team,c)\n",
        "    return\n",
        "  #Implementing the method for crop and team specified\n",
        "  team_crop_directory = Path().joinpath(dataset_name,\"Training\",team,crop)\n",
        "  for i,j in [(\"Images\",\"Images\"),(\"Masks\",\"Annotations\")]:\n",
        "    for ext in [\"*.jpg\",\"*.png\"]:\n",
        "      for path in team_crop_directory.joinpath(i).glob(ext):\n",
        "        file_destination = str(Path().joinpath(folder_name,j,path.name))\n",
        "        path.rename(file_destination)\n",
        "  if patch:\n",
        "    patchify(patch,\n",
        "             [(Path().joinpath(folder_name,\"Images\"),Path().joinpath(folder_name,\"Image_Patches\")),\n",
        "              (Path().joinpath(folder_name,\"Annotations\"),Path().joinpath(folder_name,\"Annotation_Patches\"))])\n",
        "\n",
        "def split(ratio: float = ratio,image_folder = img_folder):\n",
        "  images = Path().joinpath(folder_name,image_folder)\n",
        "  num_images = len([x for x in images.glob(\"*\")])\n",
        "  training_num = int(num_images * ratio)\n",
        "  train = Path().joinpath(folder_name,\"Splits\",\"train.txt\").open(\"a\")\n",
        "  val = Path().joinpath(folder_name,\"Splits\",\"val.txt\").open(\"a\")\n",
        "  i = 0\n",
        "  for img in images.glob(\"*\"):\n",
        "    if i < training_num:\n",
        "      train.write(img.stem + \"\\n\")\n",
        "    else:\n",
        "      val.write(img.stem + \"\\n\")\n",
        "    i += 1\n",
        "\n",
        "def patchify(patch_size,transfer_list):\n",
        "   if not patch_size:\n",
        "     return\n",
        "   for i,j in transfer_list:\n",
        "    Path().joinpath(j).mkdir(parents=True, exist_ok=True)\n",
        "    for ext in [\"*.jpg\",\"*.png\"]:\n",
        "      for path in Path().joinpath(i).glob(ext):\n",
        "        k = 0\n",
        "        img = Image.open(path)\n",
        "        if img.size[0] < patch_size or img.size[1] < patch_size:\n",
        "          print(\"Image from \" + str(path) + \" of shape \" + str(img.size) + \" too small for patch size \" + str(patch_size) + \" resizing...\")\n",
        "          img = img.resize((patch_size, patch_size))\n",
        "          \n",
        "        new_patches =patchify_overlap(np.array(img),(patch_size,patch_size),54)\n",
        "        for m in new_patches:\n",
        "          m = Image.fromarray(m)\n",
        "          m.save(Path().joinpath(j, path.stem + '_' + str(k) + path.suffix))\n",
        "          k +=  1\n",
        "        \n",
        "       # for k in range(0,img.width // patch_size):\n",
        "       #   for s in range(0,img.height // patch_size):\n",
        "       #     left = k*patch_size\n",
        "       #     upper = s*patch_size\n",
        "       #     patch = img.crop((left,upper,left+patch_size,upper+patch_size))\n",
        "       #     patch.save(Path().joinpath(j, path.stem + '_' + str(k) + '-' + str(s) + path.suffix))\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ_8ACV2DH4P"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "\n",
        "\n",
        "def window_nd(a, window, steps = None, axis = None, outlist = False):\n",
        "    ashp = np.array(a.shape)\n",
        "    if a.shape[0] < window[0] or a.shape[1] < window[1]:\n",
        "      print(\"Patch size\" + str(window) + \" too large for this picture of size \" + str(a.shape))\n",
        "      out = np.empty((1,a.shape[0],a.shape[1],3),dtype=np.uint8)\n",
        "      out[0] = a\n",
        "      return out\n",
        "      \n",
        "\n",
        "    if axis != None:\n",
        "        axs = np.array(axis, ndmin = 1)\n",
        "    else:\n",
        "        axs = np.arange(ashp.size)\n",
        "\n",
        "    window = np.array(window, ndmin = 1)\n",
        "    wshp = ashp.copy()\n",
        "    wshp[axs] = window\n",
        "\n",
        "    stp = np.ones_like(ashp)\n",
        "    if steps:\n",
        "        steps = np.array(steps, ndmin = 1)\n",
        "        stp[axs] = steps\n",
        "\n",
        "    astr = np.array(a.strides)\n",
        "\n",
        "    shape = tuple((ashp - wshp) // stp + 1) + tuple(wshp)\n",
        "    strides = tuple(astr * stp) + tuple(astr)\n",
        "\n",
        "    as_strided = np.lib.stride_tricks.as_strided\n",
        "    a_view = np.squeeze(as_strided(a, \n",
        "                                 shape = shape, \n",
        "                                 strides = strides, writeable=False))\n",
        "    if outlist:\n",
        "        return list(a_view.reshape((-1,) + tuple(wshp)))\n",
        "    else:\n",
        "        # return view (N, p_h, p_w, channels)\n",
        "        return a_view.reshape((-1,) + tuple(wshp)) #a_view\n",
        "\n",
        "\n",
        "def patchify_overlap(img, patch_shape=(1000,1000), overlap=10):\n",
        "\n",
        "    img_h, img_w = img.shape[:2]\n",
        "    p_h, p_w = patch_shape[:2]\n",
        "\n",
        "    return window_nd(img, (p_h, p_w), \n",
        "        steps=(p_h-overlap,p_w-overlap), axis=(0,1))\n",
        "\n",
        "# simple loop to collect image back with overlap\n",
        "\n",
        "def unpatchify_overlap(patches, image_size, overlap=10):    \n",
        "    img_h, img_w = image_size[:2]\n",
        "\n",
        "\n",
        "    n_p, p_h, p_w = patches.shape[:3]\n",
        "\n",
        "    n_h = (img_h - overlap) // (p_h - overlap) \n",
        "\n",
        "    if (img_h - overlap) % (p_h - overlap) > 0:\n",
        "        n_h += 1\n",
        "\n",
        "    n_w = (img_w - overlap) // (p_w - overlap)\n",
        "    \n",
        "    img = np.zeros((img_h, img_w, image_size[2]), dtype=patches.dtype)\n",
        "\n",
        "    patch_idx = 0\n",
        "\n",
        "    pos_h = 0\n",
        "\n",
        "    pos_w = 0\n",
        "\n",
        "    for i in range(n_h):\n",
        "        patch_offset_h = overlap//2 if i > 0 else 0\n",
        "\n",
        "        height_left = img_h - pos_h \n",
        "\n",
        "        h_to_insert = np.min([p_h - patch_offset_h, height_left])\n",
        "\n",
        "        for j in range(n_w):\n",
        "\n",
        "            p = patches[patch_idx]\n",
        "\n",
        "            patch_offset_w = overlap//2 if j > 0 else 0\n",
        "\n",
        "            width_left = img_w - pos_w \n",
        "\n",
        "            w_to_insert = np.min([p_w - patch_offset_w, width_left])\n",
        "\n",
        "            print('h:{}, w:{}, h_i:{}, w_i:{}'.format(pos_h, pos_w,\n",
        "\n",
        "                h_to_insert, w_to_insert))\n",
        "\n",
        "            img[pos_h:(pos_h+h_to_insert),pos_w:(pos_w+w_to_insert),:] = (\n",
        "\n",
        "                    p[patch_offset_h:(h_to_insert + patch_offset_h ),    \n",
        "\n",
        "                      patch_offset_w:(w_to_insert + patch_offset_w), :])\n",
        "\n",
        "            pos_w += w_to_insert - overlap // 2\n",
        "\n",
        "            patch_idx += 1\n",
        "\n",
        "            print('patch {}/{}'.format(patch_idx, len(patches)))\n",
        "\n",
        "            if patch_idx > len(patches) - 1:\n",
        "\n",
        "                return img\n",
        "\n",
        "        pos_w = 0    \n",
        "\n",
        "        pos_h += h_to_insert - overlap // 2\n",
        "\n",
        "    return img\n",
        "\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDBvdv9Uxay9"
      },
      "source": [
        "def run_data_config(ratio: float = ratio,team = curr_team ,crop = curr_crop ,patch = None,image_folder = img_folder):\n",
        "  build_directory_structure()\n",
        "  fill_directory(team = team, crop = crop, patch = patch)\n",
        "  split(ratio = ratio, image_folder= img_folder)\n",
        "run_data_config(patch = 864 )"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQ5ntZhxazC"
      },
      "source": [
        "#Path_folder is the folder where the predicted small masks are\n",
        "#prefix is the prefix of the names of the mask to search\n",
        "def unpatchify(full_size,patch_size,path_folder,prefix):\n",
        "  if not patch_size:\n",
        "    return\n",
        "  #img_new = Image.new(size=full_size,mode=\"RGB\")\n",
        "  patches = list(Path(path_folder).glob(prefix + '*'))\n",
        "  print(len(patches))\n",
        "  patch_array = np.empty((len(patches),patch_size,patch_size,3),dtype=np.uint8)\n",
        "  print(patch_array.shape)\n",
        "  print(\"Current unpatchify prefix: \" + prefix)\n",
        "  for patch in patches:\n",
        "    print(patches)\n",
        "    index = int(patch.stem.split('_')[-1])\n",
        "    patch_array[index] = np.array(Image.open(patch))\n",
        "  img_new = unpatchify_overlap(patch_array,full_size,54)\n",
        "  return img_new"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZfaatV7xazF"
      },
      "source": [
        "# Defining augmentation/custom dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p50055oQxazH"
      },
      "source": [
        "# ImageDataGenerator\n",
        "# ------------------\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def image_augmentation(rotation=0,w_shift=0,h_shift=0,zoom=0,h_flip=False,v_flip=False, fill_mode=\"\", rescale=1, preprocess_input=None):\n",
        "  return ImageDataGenerator(rotation_range=rotation,\n",
        "                                      width_shift_range=w_shift,\n",
        "                                      height_shift_range=h_shift,\n",
        "                                      zoom_range=zoom,\n",
        "                                      horizontal_flip=h_flip,\n",
        "                                      vertical_flip=v_flip,\n",
        "                                      fill_mode=fill_mode,\n",
        "                                      rescale=rescale,\n",
        "                                      preprocessing_function=preprocess_input)\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0RdYZxYxazH"
      },
      "source": [
        "def read_rgb_mask(img_path):\n",
        "    '''\n",
        "    img_path: path to the mask file\n",
        "    Returns the numpy array containing target values\n",
        "    '''\n",
        "\n",
        "    mask_img = Image.open(img_path)\n",
        "    mask_arr = np.array(mask_img)\n",
        "\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "\n",
        "    return new_mask_arr\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7epui-aMxazJ"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  \"\"\"\n",
        "    CustomDataset inheriting from tf.keras.utils.Sequence.\n",
        "\n",
        "    3 main methods:\n",
        "      - __init__: save dataset params like directory, filenames..\n",
        "      - __len__: return the total number of samples in the dataset\n",
        "      - __getitem__: return a sample from the dataset\n",
        "\n",
        "    Note: \n",
        "      - the custom dataset return a single sample from the dataset. Then, we use \n",
        "        a tf.data.Dataset object to group samples into batches.\n",
        "      - in this case we have a different structure of the dataset in memory. \n",
        "        We have all the images in the same folder and the training and validation splits\n",
        "        are defined in text files.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \n",
        "               preprocessing_function=None, out_shape=[img_h, img_w],img_folder = img_folder ,ann_folder = ann_folder):\n",
        "    if which_subset == 'training':\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'train.txt')\n",
        "    elif which_subset == 'validation':\n",
        "      subset_file = os.path.join(dataset_dir, 'Splits', 'val.txt')\n",
        "    \n",
        "    with open(subset_file, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "    \n",
        "    subset_filenames = []\n",
        "    for line in lines:\n",
        "      subset_filenames.append(line.strip()) \n",
        "\n",
        "    self.which_subset = which_subset\n",
        "    self.dataset_dir = dataset_dir\n",
        "    self.subset_filenames = subset_filenames\n",
        "    self.img_generator = img_generator\n",
        "    self.mask_generator = mask_generator\n",
        "    self.preprocessing_function = preprocessing_function\n",
        "    self.out_shape = out_shape\n",
        "    self.img_folder = img_folder\n",
        "    self.ann_folder = ann_folder\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.subset_filenames)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Read Image\n",
        "    curr_filename = self.subset_filenames[index]\n",
        "    img = Image.open(os.path.join(self.dataset_dir, self.img_folder, curr_filename + '.jpg'))\n",
        "    mask = Image.fromarray(read_rgb_mask(os.path.join(self.dataset_dir, self.ann_folder , curr_filename + '.png')))\n",
        "\n",
        "    # Resize image and mask\n",
        "    img = img.resize(self.out_shape)\n",
        "    mask = mask.resize(self.out_shape)\n",
        "    \n",
        "    img_arr = np.array(img)\n",
        "    mask_arr = np.array(mask)\n",
        "\n",
        "    mask_arr = np.expand_dims(mask_arr, -1)\n",
        "\n",
        "    if self.which_subset == 'training':\n",
        "      if self.img_generator is not None and self.mask_generator is not None:\n",
        "        # Perform data augmentation\n",
        "        # We can get a random transformation from the ImageDataGenerator using get_random_transform\n",
        "        # and we can apply it to the image using apply_transform\n",
        "        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\n",
        "        mask_t = self.mask_generator.get_random_transform(mask_arr.shape, seed=SEED)\n",
        "        img_arr = self.img_generator.apply_transform(img_arr, img_t)\n",
        "        # ImageDataGenerator use bilinear interpolation for augmenting the images.\n",
        "        # Thus, when applied to the masks it will output 'interpolated classes', which\n",
        "        # is an unwanted behaviour. As a trick, we can transform each class mask \n",
        "        # separately and then we can cast to integer values (as in the binary segmentation notebook).\n",
        "        # Finally, we merge the augmented binary masks to obtain the final segmentation mask.\n",
        "        out_mask = np.zeros_like(mask_arr)\n",
        "        for c in np.unique(mask_arr):\n",
        "          if c > 0:\n",
        "            curr_class_arr = np.float32(mask_arr == c)\n",
        "            curr_class_arr = self.mask_generator.apply_transform(curr_class_arr, mask_t)\n",
        "            # from [0, 1] to {0, 1}\n",
        "            curr_class_arr = np.uint8(curr_class_arr)\n",
        "            # recover original class\n",
        "            curr_class_arr = curr_class_arr * c \n",
        "            out_mask += curr_class_arr\n",
        "    else:\n",
        "      out_mask = mask_arr\n",
        "    \n",
        "    if self.preprocessing_function is not None:\n",
        "        img_arr = self.preprocessing_function(img_arr)\n",
        "\n",
        "    return img_arr, np.float32(out_mask)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0iuM-rhxazL"
      },
      "source": [
        "#Instantiating custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcmxK8ROxazM"
      },
      "source": [
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "dataset = CustomDataset(Path().joinpath(folder_name), 'training', \n",
        "                        img_generator=image_augmentation(rotation=30, \n",
        "                                                         w_shift=30, \n",
        "                                                         h_shift=30, \n",
        "                                                         zoom=0.1, \n",
        "                                                         h_flip=True, \n",
        "                                                         v_flip=True, \n",
        "                                                         fill_mode='reflect',\n",
        "                                                         preprocess_input=tf.keras.applications.vgg16.preprocess_input), \n",
        "                        mask_generator=image_augmentation(rotation=30, \n",
        "                                                          w_shift=30, \n",
        "                                                          h_shift=30, \n",
        "                                                          zoom=0.1, \n",
        "                                                          h_flip=True, \n",
        "                                                          v_flip=True, \n",
        "                                                          fill_mode='reflect',\n",
        "                                                          preprocess_input=tf.keras.applications.vgg16.preprocess_input))\n",
        "dataset_valid = CustomDataset(Path().joinpath(folder_name),\n",
        "                              'validation',)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL110aNCxazN"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_generator(lambda: dataset,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_w, img_h, 3], [img_w, img_h, 1]))\n",
        "\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: dataset_valid,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([img_w, img_h, 3], [img_w,img_h, 1]))\n",
        "valid_dataset = valid_dataset.batch(batch_size)\n",
        "\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh60Us_Lu5kN"
      },
      "source": [
        "# Renet Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXRqm9xau_Kd"
      },
      "source": [
        "from tensorflow.keras.layers import Permute, LSTM, Bidirectional, Lambda, UpSampling2D\n",
        "\n",
        "class RenetLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "  def __init__(self,patch_size,n_hidden,stack_sublayers,batch_size,shape,regularizer):\n",
        "    super(RenetLayer,self).__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.n_hidden = n_hidden\n",
        "    self.stack_sublayers = stack_sublayers\n",
        "    self.batch_size = batch_size\n",
        "    self.input_layer = None\n",
        "    self.regularizer = regularizer\n",
        "    self.shape = shape\n",
        "    _, cwidth, cheight, cchannels = self.shape\n",
        "    pwidth, pheight, pchannels = self.patch_size\n",
        "    psize = pheight * pwidth * pchannels\n",
        "    npatchesH = cheight // pheight\n",
        "    npatchesW = cwidth // pwidth\n",
        "    self.patch_size = list(self.patch_size)\n",
        "    self.patch_size.insert(0, 1)\n",
        "    self.patch_size[-1] = 1\n",
        "    self.tf_patches = Lambda(lambda x: tf.image.extract_patches(x,sizes=self.patch_size, strides=self.patch_size, rates=[1, 1, 1, 1], padding=\"SAME\"))\n",
        "    self.l_sub0 = Lambda(lambda x: tf.reshape(x,(self.batch_size,npatchesW, psize))) \n",
        "    # LSTM takes 3d tensor (batch, timesteps, features)\n",
        "    \n",
        "    self.l_bidir0_forward = LSTM(n_hidden, return_sequences=True, kernel_regularizer=self.regularizer)\n",
        "    self.l_bidir0_backward = LSTM(n_hidden, return_sequences=True, kernel_regularizer=self.regularizer)\n",
        "    self.l_bidir0_concat = Concatenate(axis = 2)\n",
        "    \n",
        "    self.l_sub1_l = Lambda(lambda x: tf.reshape(x,(self.batch_size, npatchesH, 2*n_hidden)))\n",
        "    \n",
        "    self.l_bidir1_forward = LSTM(n_hidden, return_sequences=True, kernel_regularizer=self.regularizer)\n",
        "    self.l_bidir1_backward = LSTM(n_hidden, return_sequences=True, kernel_regularizer=self.regularizer)\n",
        "    self.l_bidir1_concat = Concatenate(axis = 2)\n",
        "\n",
        "    self.l_sub1_bil = Lambda(lambda x: tf.reshape(x,(self.batch_size, npatchesW, npatchesH, 2 * self.n_hidden)))\n",
        "  def call(self, inputs):\n",
        "    x = self.tf_patches(inputs)\n",
        "    x = self.l_sub0(x)\n",
        "    \n",
        "    y = self.l_bidir0_forward(x)\n",
        "    z = self.l_bidir0_backward(y)\n",
        "    x = self.l_bidir0_concat([y, z])\n",
        "    \n",
        "    x = self.l_sub1_l(x)\n",
        "\n",
        "    y = self.l_bidir1_forward(x)\n",
        "    z = self.l_bidir1_backward(y)\n",
        "    x = self.l_bidir1_concat([y, z])\n",
        "\n",
        "    x = self.l_sub1_bil(x)\n",
        "    return x\n",
        "  \n",
        "  def get_config(self):\n",
        "    return {\"patch_size\":self.patch_size,\n",
        "            \"n_hidden\": self.n_hidden,\n",
        "            \"stack_sublayers\": self.stack_sublayers,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"shape\": self.shape,\n",
        "            \"regularizer\":self.regularizer}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7oAC9iQvJs9"
      },
      "source": [
        "# Unet Backbone"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZxJPnJcvLzZ"
      },
      "source": [
        "def get_norm_activation(function, y):\n",
        "  t = BatchNormalization()(y)\n",
        "  t = Activation(function)(t)\n",
        "  return t\n",
        "\n",
        "def get_convolution_layer(x, filters, kernel_regularizer=None):\n",
        "  z = Conv2D(filters=filters,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer)(x)\n",
        "  z = get_norm_activation(\"relu\", z)\n",
        "  z = Conv2D(filters=filters,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer)(z)\n",
        "  z = get_norm_activation(\"relu\", z)\n",
        "  return z\n",
        "\n",
        "def get_decoding_layer(x, filters, kernel_regularizer=None):\n",
        "  y = Conv2D(filters=filters,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer)(x)\n",
        "  y = get_norm_activation(\"relu\", y)\n",
        "  y = Conv2D(filters=filters,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer)(y)\n",
        "  y = get_norm_activation(\"relu\", y)\n",
        "  y = Conv2DTranspose(filters=filters//2,kernel_size=(2,2),padding='same',strides=2)(y)\n",
        "  return y\n",
        "\n",
        "def get_down_and_crop(x, cropped):\n",
        "  cropped.append(Cropping2D(cropping=((0,0)))(x))\n",
        "  s = MaxPool2D(pool_size=(2,2))(x)\n",
        "  return s, cropped\n",
        "\n",
        "def get_encoding_layer(x, filters, cropped, kernel_regularizer=None):\n",
        "  m = get_convolution_layer(x, filters, kernel_regularizer)\n",
        "  m, cropped = get_down_and_crop(m, cropped)\n",
        "  return m, cropped\n",
        "\n",
        "def build_unet(start_f, depth,size, kernel_regularizer=None, p_dropout=0):\n",
        "  cropped = []\n",
        "  inputs = tf.keras.Input(shape=(size[0],size[1],3))\n",
        "\n",
        "  # Encoding\n",
        "  for i in range(0, depth):\n",
        "    x, cropped = get_encoding_layer(inputs if i==0 else x, start_f, cropped, kernel_regularizer)\n",
        "    start_f *= 2\n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "  \n",
        "  x = Dropout(p_dropout)(x) #performs implicit data augmentation\n",
        "\n",
        "  # Decoding\n",
        "  start_f = start_f //2\n",
        "  print(cropped)\n",
        "  x = Conv2DTranspose(filters=start_f,kernel_size=(2,2),padding='same',strides=2)(x)\n",
        "  x = Concatenate()([cropped[-1],x])\n",
        "\n",
        "  for j in range(1, depth):\n",
        "    x = get_decoding_layer(x, start_f, kernel_regularizer)\n",
        "    x = Concatenate()([cropped[-(j + 1)],x])\n",
        "    start_f = start_f // 2\n",
        "  \n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "\n",
        "  outputs = Conv2D(filters=3,kernel_size=(1,1),use_bias=False,activation='softmax')(x)\n",
        "  return tf.keras.Model(inputs,outputs),inputs,outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va8RPsyAxazW"
      },
      "source": [
        "# UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNIlZnEBxazX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e35fe10f-71a1-497b-8139-3ae94905fc54"
      },
      "source": [
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "model,_,_ = build_unet(32, 5,[288,288], tf.keras.regularizers.l2(1e-4), 0)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 288, 288, 32) dtype=float32 (created by layer 'cropping2d')>, <KerasTensor: shape=(None, 144, 144, 64) dtype=float32 (created by layer 'cropping2d_1')>, <KerasTensor: shape=(None, 72, 72, 128) dtype=float32 (created by layer 'cropping2d_2')>, <KerasTensor: shape=(None, 36, 36, 256) dtype=float32 (created by layer 'cropping2d_3')>, <KerasTensor: shape=(None, 18, 18, 512) dtype=float32 (created by layer 'cropping2d_4')>]\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 288, 288, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 288, 288, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 288, 288, 32) 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 288, 288, 32) 0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 288, 288, 32) 9216        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 288, 288, 32) 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 288, 288, 32) 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 144, 144, 32) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 144, 144, 64) 18432       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 144, 144, 64) 256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 144, 144, 64) 0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 144, 144, 64) 36864       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 144, 144, 64) 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 144, 144, 64) 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 72, 72, 64)   0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 72, 72, 128)  73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 72, 72, 128)  512         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 72, 72, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 72, 72, 128)  147456      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 72, 72, 128)  512         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 72, 72, 128)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 36, 36, 128)  0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 36, 36, 256)  294912      max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 36, 36, 256)  1024        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 36, 36, 256)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 36, 36, 256)  589824      activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 36, 36, 256)  1024        conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 36, 36, 256)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 256)  0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 18, 18, 512)  1179648     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 18, 18, 512)  2048        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 18, 18, 512)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 18, 18, 512)  2359296     activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 18, 18, 512)  2048        conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 18, 18, 512)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 9, 9, 512)    0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 9, 9, 1024)   4718592     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 9, 9, 1024)   4096        conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 9, 9, 1024)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 9, 9, 1024)   9437184     activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 9, 9, 1024)   4096        conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 9, 9, 1024)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 9, 9, 1024)   0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_4 (Cropping2D)       (None, 18, 18, 512)  0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose (Conv2DTranspo (None, 18, 18, 512)  2097664     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 18, 18, 1024) 0           cropping2d_4[0][0]               \n",
            "                                                                 conv2d_transpose[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 18, 18, 512)  4718592     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 18, 18, 512)  2048        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 18, 18, 512)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 18, 18, 512)  2359296     activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 18, 18, 512)  2048        conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 18, 18, 512)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_3 (Cropping2D)       (None, 36, 36, 256)  0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTrans (None, 36, 36, 256)  524544      activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 36, 36, 512)  0           cropping2d_3[0][0]               \n",
            "                                                                 conv2d_transpose_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 36, 36, 256)  1179648     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 36, 36, 256)  1024        conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 36, 36, 256)  0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 36, 36, 256)  589824      activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 36, 36, 256)  1024        conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 36, 36, 256)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_2 (Cropping2D)       (None, 72, 72, 128)  0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_2 (Conv2DTrans (None, 72, 72, 128)  131200      activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 72, 72, 256)  0           cropping2d_2[0][0]               \n",
            "                                                                 conv2d_transpose_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 72, 72, 128)  294912      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 72, 72, 128)  512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 72, 72, 128)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 72, 72, 128)  147456      activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 72, 72, 128)  512         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 72, 72, 128)  0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_1 (Cropping2D)       (None, 144, 144, 64) 0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_3 (Conv2DTrans (None, 144, 144, 64) 32832       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 144, 144, 128 0           cropping2d_1[0][0]               \n",
            "                                                                 conv2d_transpose_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 144, 144, 64) 73728       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 144, 144, 64) 256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 144, 144, 64) 0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 144, 144, 64) 36864       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 144, 144, 64) 256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 144, 144, 64) 0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d (Cropping2D)         (None, 288, 288, 32) 0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_4 (Conv2DTrans (None, 288, 288, 32) 8224        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 288, 288, 64) 0           cropping2d[0][0]                 \n",
            "                                                                 conv2d_transpose_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 288, 288, 32) 18432       concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 288, 288, 32) 128         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 288, 288, 32) 0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 288, 288, 32) 9216        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 288, 288, 32) 128         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 288, 288, 32) 0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 288, 288, 3)  96          activation_21[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 31,112,608\n",
            "Trainable params: 31,100,576\n",
            "Non-trainable params: 12,032\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRg73OI0tCox"
      },
      "source": [
        "#DuccioNet: Apri tutto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph6aovCAtGJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3126b101-5225-4686-c2c9-cc60ed83d214"
      },
      "source": [
        "def build_unet_renet(start_f, depth,size, kernel_regularizer=None, p_dropout=0):\n",
        "  cropped = []\n",
        "  inputs = tf.keras.Input(shape=(size[0],size[1],3))\n",
        "\n",
        "  # Encoding\n",
        "  for i in range(0, depth):\n",
        "    x, cropped = get_encoding_layer(inputs if i==0 else x, start_f, cropped, kernel_regularizer)\n",
        "    start_f *= 2\n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "  x = Dropout(p_dropout)(x) \n",
        "  x = RenetLayer((2, 2,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "  x = RenetLayer((1,1,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "  x = RenetLayer((1,1,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "                                              \n",
        "  # Decoding\n",
        "  start_f = start_f //2\n",
        "  print(cropped)\n",
        "  x = Conv2DTranspose(filters=start_f,kernel_size=(2,2),padding='same',strides=2)(x)\n",
        "  x = Conv2DTranspose(filters=start_f,kernel_size=(2,2),padding='same',strides=2)(x)\n",
        "  x = Concatenate()([cropped[-1],x])\n",
        "\n",
        "  for j in range(1, depth):\n",
        "    x = get_decoding_layer(x, start_f, kernel_regularizer)\n",
        "    x = Concatenate()([cropped[-(j + 1)],x])\n",
        "    start_f = start_f // 2\n",
        "  \n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "\n",
        "  outputs = Conv2D(filters=3,kernel_size=(1,1),use_bias=False,activation='softmax')(x)\n",
        "  return tf.keras.Model(inputs,outputs),inputs,outputs\n",
        "\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "patch_learning = True\n",
        "if patch_learning:\n",
        "  change_config(new_patch_size=864, new_batch_size=4)\n",
        "  model,_,_ = build_unet_renet(32, 4,[curr_patch_size,curr_patch_size], tf.keras.regularizers.l2(1e-4), 0.5)\n",
        "else:\n",
        "  model,_,_ = build_unet_renet(32, 4,[img_h,img_w], tf.keras.regularizers.l2(1e-4), 0.5)\n",
        "model.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 864, 864, 32) dtype=float32 (created by layer 'cropping2d_9')>, <KerasTensor: shape=(None, 432, 432, 64) dtype=float32 (created by layer 'cropping2d_10')>, <KerasTensor: shape=(None, 216, 216, 128) dtype=float32 (created by layer 'cropping2d_11')>, <KerasTensor: shape=(None, 108, 108, 256) dtype=float32 (created by layer 'cropping2d_12')>]\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 864, 864, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 864, 864, 32) 864         input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 864, 864, 32) 128         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 864, 864, 32) 0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 864, 864, 32) 9216        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 864, 864, 32) 128         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 864, 864, 32) 0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 432, 432, 32) 0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 432, 432, 64) 18432       max_pooling2d_9[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 432, 432, 64) 256         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 432, 432, 64) 0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 432, 432, 64) 36864       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 432, 432, 64) 256         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 432, 432, 64) 0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 216, 216, 64) 0           activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 216, 216, 128 73728       max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 216, 216, 128 512         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 216, 216, 128 0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 216, 216, 128 147456      activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 216, 216, 128 512         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 216, 216, 128 0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 108, 108, 128 0           activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 108, 108, 256 294912      max_pooling2d_11[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 108, 108, 256 1024        conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 108, 108, 256 0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 108, 108, 256 589824      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 108, 108, 256 1024        conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 108, 108, 256 0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling2D) (None, 54, 54, 256)  0           activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 54, 54, 512)  1179648     max_pooling2d_12[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 54, 54, 512)  2048        conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 54, 54, 512)  0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 54, 54, 512)  2359296     activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 54, 54, 512)  2048        conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 54, 54, 512)  0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 54, 54, 512)  0           activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_3 (RenetLayer)      (None, 27, 27, 512)  4198400     dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_4 (RenetLayer)      (None, 27, 27, 512)  2625536     renet_layer_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_5 (RenetLayer)      (None, 27, 27, 512)  2625536     renet_layer_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_10 (Conv2DTran (None, 54, 54, 256)  524544      renet_layer_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_12 (Cropping2D)      (None, 108, 108, 256 0           activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_11 (Conv2DTran (None, 108, 108, 256 262400      conv2d_transpose_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 108, 108, 512 0           cropping2d_12[0][0]              \n",
            "                                                                 conv2d_transpose_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 108, 108, 256 1179648     concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 108, 108, 256 1024        conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 108, 108, 256 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 108, 108, 256 589824      activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 108, 108, 256 1024        conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 108, 108, 256 0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_11 (Cropping2D)      (None, 216, 216, 128 0           activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_12 (Conv2DTran (None, 216, 216, 128 131200      activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 216, 216, 256 0           cropping2d_11[0][0]              \n",
            "                                                                 conv2d_transpose_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 216, 216, 128 294912      concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 216, 216, 128 512         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 216, 216, 128 0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 216, 216, 128 147456      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 216, 216, 128 512         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 216, 216, 128 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_10 (Cropping2D)      (None, 432, 432, 64) 0           activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_13 (Conv2DTran (None, 432, 432, 64) 32832       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 432, 432, 128 0           cropping2d_10[0][0]              \n",
            "                                                                 conv2d_transpose_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 432, 432, 64) 73728       concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 432, 432, 64) 256         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 432, 432, 64) 0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 432, 432, 64) 36864       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 432, 432, 64) 256         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 432, 432, 64) 0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_9 (Cropping2D)       (None, 864, 864, 32) 0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_14 (Conv2DTran (None, 864, 864, 32) 8224        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 864, 864, 64) 0           cropping2d_9[0][0]               \n",
            "                                                                 conv2d_transpose_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 864, 864, 32) 18432       concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 864, 864, 32) 128         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 864, 864, 32) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 864, 864, 32) 9216        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 864, 864, 32) 128         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 864, 864, 32) 0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 864, 864, 3)  96          activation_57[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 17,480,864\n",
            "Trainable params: 17,474,976\n",
            "Non-trainable params: 5,888\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKHIo_2gPKYT"
      },
      "source": [
        "# FrebianiNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EotovzpPNpV",
        "outputId": "0df2598f-bf81-4cdd-9a63-8a658b8e14db"
      },
      "source": [
        "def build_unet_frebiani(start_f, depth,size, kernel_regularizer=None, p_dropout=0):\n",
        "  cropped = []\n",
        "  inputs = tf.keras.Input(shape=(size[0],size[1],3))\n",
        "\n",
        "  # Encoding\n",
        "  for i in range(0, depth):\n",
        "    x, cropped = get_encoding_layer(inputs if i==0 else x, start_f, cropped, kernel_regularizer)\n",
        "    start_f *= 2\n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "  x = Dropout(p_dropout)(x) \n",
        "  y = Conv2D(filters=start_f,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer,dilation_rate=(2,2))(x)\n",
        "  y = Conv2D(filters=start_f ,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer,dilation_rate=(2,2))(y)\n",
        "  y = Conv2D(filters=start_f,kernel_size=(3,3),use_bias=False,padding='same', kernel_regularizer=kernel_regularizer,dilation_rate=(2,2))(y)\n",
        "  x = RenetLayer((2, 2,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "  x = RenetLayer((1,1,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "  x = RenetLayer((1,1,512),256,True,-1,x.shape,kernel_regularizer)(x)\n",
        "  x = Conv2DTranspose(filters=start_f,kernel_size=(2,2),padding='same',strides=2)(x)\n",
        "  x = Concatenate()([y,x])                                           \n",
        "  # Decoding\n",
        "  start_f = start_f //2\n",
        "  print(cropped)\n",
        "  x = Conv2DTranspose(filters=start_f,kernel_size=(2,2),padding='same',strides=2)(x)\n",
        "  x = Concatenate()([cropped[-1],x])\n",
        "\n",
        "  for j in range(1, depth):\n",
        "    x = get_decoding_layer(x, start_f, kernel_regularizer)\n",
        "    x = Concatenate()([cropped[-(j + 1)],x])\n",
        "    start_f = start_f // 2\n",
        "  \n",
        "  x = get_convolution_layer(x, start_f, kernel_regularizer)\n",
        "\n",
        "  outputs = Conv2D(filters=3,kernel_size=(1,1),use_bias=False,activation='softmax')(x)\n",
        "  return tf.keras.Model(inputs,outputs),inputs,outputs\n",
        "\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "patch_learning = True\n",
        "change_config(new_patch_size=None)\n",
        "if patch_learning:\n",
        "  model,_,_ = build_unet_frebiani(32, 4,[curr_patch_size,curr_patch_size], tf.keras.regularizers.l2(1e-4), 0.5)\n",
        "else:\n",
        "  model,_,_ = build_unet_frebiani(32, 4,[img_w,img_h], tf.keras.regularizers.l2(1e-4), 0.5)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 864, 864, 32) dtype=float32 (created by layer 'cropping2d_18')>, <KerasTensor: shape=(None, 432, 432, 64) dtype=float32 (created by layer 'cropping2d_19')>, <KerasTensor: shape=(None, 216, 216, 128) dtype=float32 (created by layer 'cropping2d_20')>, <KerasTensor: shape=(None, 108, 108, 256) dtype=float32 (created by layer 'cropping2d_21')>]\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 864, 864, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 864, 864, 32) 864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 864, 864, 32) 128         conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 864, 864, 32) 0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 864, 864, 32) 9216        activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 864, 864, 32) 128         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 864, 864, 32) 0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling2D) (None, 432, 432, 32) 0           activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 432, 432, 64) 18432       max_pooling2d_18[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 432, 432, 64) 256         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 432, 432, 64) 0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 432, 432, 64) 36864       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 432, 432, 64) 256         conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 432, 432, 64) 0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling2D) (None, 216, 216, 64) 0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 216, 216, 128 73728       max_pooling2d_19[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 216, 216, 128 512         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 216, 216, 128 0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 216, 216, 128 147456      activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 216, 216, 128 512         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 216, 216, 128 0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling2D) (None, 108, 108, 128 0           activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 108, 108, 256 294912      max_pooling2d_20[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 108, 108, 256 1024        conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 108, 108, 256 0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 108, 108, 256 589824      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 108, 108, 256 1024        conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 108, 108, 256 0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling2D) (None, 54, 54, 256)  0           activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 54, 54, 512)  1179648     max_pooling2d_21[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 54, 54, 512)  2048        conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 54, 54, 512)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 54, 54, 512)  2359296     activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 54, 54, 512)  2048        conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 54, 54, 512)  0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 54, 54, 512)  0           activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_6 (RenetLayer)      (None, 27, 27, 512)  4198400     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 54, 54, 512)  2359296     dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_7 (RenetLayer)      (None, 27, 27, 512)  2625536     renet_layer_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 54, 54, 512)  2359296     conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "renet_layer_8 (RenetLayer)      (None, 27, 27, 512)  2625536     renet_layer_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 54, 54, 512)  2359296     conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_20 (Conv2DTran (None, 54, 54, 512)  1049088     renet_layer_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 54, 54, 1024) 0           conv2d_102[0][0]                 \n",
            "                                                                 conv2d_transpose_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_21 (Cropping2D)      (None, 108, 108, 256 0           activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_21 (Conv2DTran (None, 108, 108, 256 1048832     concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 108, 108, 512 0           cropping2d_21[0][0]              \n",
            "                                                                 conv2d_transpose_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 108, 108, 256 1179648     concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 108, 108, 256 1024        conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 108, 108, 256 0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 108, 108, 256 589824      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 108, 108, 256 1024        conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 108, 108, 256 0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_20 (Cropping2D)      (None, 216, 216, 128 0           activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_22 (Conv2DTran (None, 216, 216, 128 131200      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 216, 216, 256 0           cropping2d_20[0][0]              \n",
            "                                                                 conv2d_transpose_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 216, 216, 128 294912      concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 216, 216, 128 512         conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 216, 216, 128 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 216, 216, 128 147456      activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 216, 216, 128 512         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 216, 216, 128 0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_19 (Cropping2D)      (None, 432, 432, 64) 0           activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_23 (Conv2DTran (None, 432, 432, 64) 32832       activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 432, 432, 128 0           cropping2d_19[0][0]              \n",
            "                                                                 conv2d_transpose_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 432, 432, 64) 73728       concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 432, 432, 64) 256         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 432, 432, 64) 0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 432, 432, 64) 36864       activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 432, 432, 64) 256         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 432, 432, 64) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "cropping2d_18 (Cropping2D)      (None, 864, 864, 32) 0           activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_24 (Conv2DTran (None, 864, 864, 32) 8224        activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 864, 864, 64) 0           cropping2d_18[0][0]              \n",
            "                                                                 conv2d_transpose_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 864, 864, 32) 18432       concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 864, 864, 32) 128         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 864, 864, 32) 0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 864, 864, 32) 9216        activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 864, 864, 32) 128         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 864, 864, 32) 0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 864, 864, 3)  96          activation_97[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 25,869,728\n",
            "Trainable params: 25,863,840\n",
            "Non-trainable params: 5,888\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU-yNtpoxazm"
      },
      "source": [
        "# Model Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7_1-_yKxazn"
      },
      "source": [
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = SparseCategoricalFocalLoss(gamma=2)\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "optimizer = tfa.optimizers.LazyAdam(\n",
        "        learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Here we define the intersection over union for each class in the batch.\n",
        "# Then we compute the final iou as the mean over classes\n",
        "def meanIoU(y_true, y_pred):\n",
        "    # get predicted class from softmax\n",
        "    y_pred = tf.expand_dims(tf.argmax(y_pred, -1), -1)\n",
        "\n",
        "    per_class_iou = []\n",
        "\n",
        "    for i in range(1,3): # exclude the background class 0\n",
        "      # Get prediction and target related to only a single class (i)\n",
        "      class_pred = tf.cast(tf.where(y_pred == i, 1, 0), tf.float32)\n",
        "      class_true = tf.cast(tf.where(y_true == i, 1, 0), tf.float32)\n",
        "      intersection = tf.reduce_sum(class_true * class_pred)\n",
        "      union = tf.reduce_sum(class_true) + tf.reduce_sum(class_pred) - intersection\n",
        "    \n",
        "      iou = (intersection + 1e-7) / (union + 1e-7)\n",
        "      per_class_iou.append(iou)\n",
        "\n",
        "    return tf.reduce_mean(per_class_iou)\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "metrics = ['accuracy', meanIoU]\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S529wBAuxazo"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1WcAY6Yxazq"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "exps_dir = Path(\"/\").joinpath(\"content\", \"drive\", \"MyDrive\", \"Colab Notebooks\", \"Homework2\",\"Results\")\n",
        "exps_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'UNet'\n",
        "\n",
        "exp_dir = Path(exps_dir).joinpath(model_name + '_' + str(now))\n",
        "exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = Path(exp_dir).joinpath('ckpts')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = Path(exp_dir).joinpath('tb_logs')\n",
        "tb_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "# Learning Rate Annhealing\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_meanIoU', patience=7, verbose=1, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "lr_scheduling = True\n",
        "if lr_scheduling:\n",
        "    callbacks.append(learning_rate_reduction)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SSZ9ODWxazq"
      },
      "source": [
        "# Model fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZ_wodzxazt"
      },
      "source": [
        "print_config()\n",
        "steps_train = len(dataset) // batch_size \n",
        "steps_val = len(dataset_valid) // batch_size \n",
        "model.fit(x=train_dataset,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=steps_train,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=steps_val, \n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaJfBddAxazv"
      },
      "source": [
        "# Load from checkpoint, set correct checkpoint number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-xxfaXtxazw"
      },
      "source": [
        "model.load_weights(Path(\"/\").joinpath(\"content\", \"drive\", \"MyDrive\", \"Colab Notebooks\", \"Homework2\", \"Results\", \"UNet_Dec11_18-32-03\", \"ckpts\", \"cp_11.ckpt\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiclb9Bxazy"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXdHhUaTxaz4"
      },
      "source": [
        "import json\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - foreground, 0 - background\n",
        "    Returns run length as string formatted\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def read_rgb_mask(img_path):\n",
        "    '''\n",
        "    img_path: path to the mask file\n",
        "    Returns the numpy array containing target values\n",
        "    '''\n",
        "\n",
        "    mask_img = Image.open(img_path)\n",
        "    mask_arr = np.array(mask_img)\n",
        "\n",
        "    new_mask_arr = np.zeros(mask_arr.shape[:2], dtype=mask_arr.dtype)\n",
        "\n",
        "    # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 124, 18], axis=-1))] = 0\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [255, 255, 255], axis=-1))] = 1\n",
        "    new_mask_arr[np.where(np.all(mask_arr == [216, 67, 82], axis=-1))] = 2\n",
        "\n",
        "    return new_mask_arr\n",
        "def predict(img,width,height):\n",
        "  image_array = np.array(img, dtype=np.float32).transpose((1, 0, 2))\n",
        "  out_sigmoid = model.predict(x=tf.expand_dims(image_array, 0))\n",
        "  predicted_class = tf.argmax(out_sigmoid, -1)\n",
        "  predicted_class = predicted_class[0, ...]\n",
        "  prediction = np.zeros([width, height, 3])\n",
        "  prediction[np.where(predicted_class == 0)] = [0, 0, 0]\n",
        "  prediction[np.where(predicted_class == 1)] = [255, 255, 255]\n",
        "  prediction[np.where(predicted_class == 2)] = [216, 67, 82]\n",
        "  prediction = Image.fromarray(np.array(prediction, dtype=np.uint8).transpose((1, 0, 2))) \n",
        "  return prediction\n",
        "\n",
        "def build_json(skip_teams = {}):\n",
        "    #Saving one mask to check it\n",
        "    counter = 0\n",
        "    submission_dict = {}\n",
        "    #Creating folder for downsampled masks\n",
        "    downsampled_masks = Path().joinpath('Results',\"Downsampled_Masks\")\n",
        "    downsampled_masks.mkdir(parents=True, exist_ok=True)\n",
        "    img_patches = Path().joinpath('Test_Dev',\"Image_Patches\")\n",
        "    img_patches.mkdir(parents=True, exist_ok=True)\n",
        "    for team_folder in [f for f in Path().joinpath(\"Development_Dataset\", \"Test_Dev\").iterdir() if f.is_dir()]:\n",
        "        for crop_folder in [f for f in Path(team_folder).iterdir() if f.is_dir()]:\n",
        "          img_folder = Path(crop_folder).joinpath(\"Images\")\n",
        "          patchify(curr_patch_size,[(img_folder,img_patches)])\n",
        "          for img_file in img_folder.iterdir():\n",
        "              submission_dict[img_file.stem] = {}\n",
        "              if curr_patch_size:\n",
        "                width, height = Image.open(img_file).size\n",
        "                if width < curr_patch_size or height < curr_patch_size:\n",
        "                  prediction = np.zeros((width, height, 3), dtype=np.uint8)\n",
        "                else:\n",
        "                  for img_patch in img_patches.glob(img_file.stem + \"*\"):\n",
        "                    img_patch_opened = Image.open(img_patch)\n",
        "                    width_p, height_p = img_patch_opened.size\n",
        "                    prediction = predict(img_patch_opened, width_p, height_p)\n",
        "                    prediction.save(downsampled_masks.joinpath(img_patch.stem + \".png\"))\n",
        "                  prediction = unpatchify((height,width, 3),curr_patch_size,downsampled_masks,img_file.stem)\n",
        "              else:\n",
        "                img = Image.open(img_file)\n",
        "                o_width , o_height = img.size\n",
        "                img = img.resize((img_w,img_h))\n",
        "                width,height = img.size\n",
        "                print(\"Image size in input to predict is: \" + str(img.size))\n",
        "                prediction = predict(img,width,height)\n",
        "                prediction = prediction.resize((o_width,o_height))\n",
        "                prediction = np.array(prediction)\n",
        "              print(\"Prediction shape: \" + str(prediction.shape))\n",
        "              new_mask_arr = np.zeros(prediction.shape[:2], dtype=prediction.dtype)\n",
        "              # Use RGB dictionary in 'RGBtoTarget.txt' to convert RGB to target\n",
        "              new_mask_arr[np.where(np.all(prediction == [216, 124, 18], axis=-1))] = 0\n",
        "              new_mask_arr[np.where(np.all(prediction == [255, 255, 255], axis=-1))] = 1\n",
        "              new_mask_arr[np.where(np.all(prediction == [216, 67, 82], axis=-1))] = 2\n",
        "              submission_dict[img_file.stem][\"shape\"] = (width, height)\n",
        "              submission_dict[img_file.stem][\"team\"] = team_folder.name\n",
        "              submission_dict[img_file.stem][\"crop\"] = crop_folder.name\n",
        "              submission_dict[img_file.stem]['segmentation'] = {}\n",
        "              # RLE encoding\n",
        "              # crop\n",
        "              rle_encoded_crop = rle_encode(new_mask_arr == 1)\n",
        "              # weed\n",
        "              rle_encoded_weed = rle_encode(new_mask_arr == 2)\n",
        "              submission_dict[img_file.stem]['segmentation']['crop'] = rle_encoded_crop\n",
        "              submission_dict[img_file.stem]['segmentation']['weed'] = rle_encoded_weed\n",
        "\n",
        "    Path().joinpath(\"predictions\").mkdir(parents=True, exist_ok=True)\n",
        "    submission_path = Path().joinpath(\"predictions\", \"submission.json\")\n",
        "    json.dump(submission_dict, Path(submission_path).open(\"w+\"))\n",
        "\n",
        "build_json()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}